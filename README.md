## The Nostradamus UCI Chess Engine

![Logo](https://github.com/david-carteau/nostradamus/blob/main/v0.1%20(decoder)/nostradamus.jpg)

The **Nostradamus** UCI chess engine uses **language models** inspired techniques to play chess:
- unlike other engines, it does not rely on a traditional search tree to find the optimal sequence of moves
- instead, it uses a network architecture based on [Transformers](https://arxiv.org/abs/1706.03762) to predict the next best move given a specific position

The engine is intended for **educational purposes** and is currently very weak (see below).

<br/>

## Estimated performance

| Rank | Name               | Elo  | +/- | Games | Score | Draw  |
| ---: | :----------------- | ---: | --: | ----: | ----: | ----: |
| 1    | Orion 1.0          | 936  | 20  | 1600  | 87.3% | 17.5% |
| 2    | Nostradamus 0.4    | 869  | 18  | 1600  | 82.4% | 22.0% |
| 3    | Nostradamus 0.3    | 782  | 16  | 1600  | 73.9% | 22.9% |
| 4    | Nostradamus 0.2    | 675  | 15  | 1600  | 60.5% | 28.2% |
| 5    | Nostradamus 0.1    | 618  | 15  | 1600  | 52.4% | 27.1% |
| 6    | Capture 1.0        | 473  | 16  | 1600  | 32.4% | 20.0% |
| 7    | POS 1.20           | 417  | 16  | 1600  | 25.8% | 22.0% |
| 8    | Cerebrum 1.0       | 400  | 14  | 1600  | 23.6% | 35.8% |
| 9    | Random 1.0         | 250  | 18  | 1600  | 11.7% | 23.1% |

<br/>

Testing conditions:
- _Hert500.pgn_ book, 100 repeated games by engine, **depth = 1**
- Capture 1.0 and Random 1.0 were specifically developed to evaluate the Nostradamus engine
- Anchor: POS is ranked 417 Elo on [CCRL 40/2 Archive](https://www.computerchess.org.uk/ccrl/402.archive/)

<br/>

## History

I'm fascinated by the performance of small language models such as [Microsoft Phi 3.5](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) and have decided to train one to play chess ;-)

Given a sentence, language models try to predict the most likely word to follow. Actually, they do not work with words, but with "tokens", which can be seen as "subwords".

I first trained a model with "sentences" consisting of simple sequences of moves, e.g. `e2e4 c7c5 g1f3 e7e6 d2d4 c5d4 f3d4 a7a6 f1d3`, with the aim of predicting the next best move, e.g. `g8f6`.

This is very similar to the way language models work. The advantage is that you can ask the model to predict the following moves and get a full principal variation !

However, with this approach it's very difficult for the model to know the position of the pieces: it has to follow every piece from the beginning of the game (e.g. after `... g1f3 ... f3d4` : there is now a knight on `d4`). This leads to a lot of illegal moves when trying to predict the next best move.

Instead of the sequence of moves, I decided to give the (textual) representation of the board (as a fenstring) as the "sentence": this drastically reduces the generation of illegal moves !

This gave me **v0.1**, based on the [Microsoft Phi 3.5](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) model architecture (decoder-only architecture).

I then realised that given a position, trying to predict the best move to play could be seen as... a translation problem !

I switched to such a specialised model (encoder-decoder architecture), [Google T5](https://huggingface.co/google-t5/t5-base), which gave me the **v0.2** and a nice improvement in strength !

I was then interested in delving into the [Transformers](https://arxiv.org/abs/1706.03762) architecture, and implemented a first (and very simple) custom network in **v0.3** (encoder-only architecture).

To avoid losing too much strength with random moves, I tried reducing the number of illegal moves generated by the network with **v0.4**.

<br/>

## How it works

Current version (v0.4):
- takes a fenstring as input
- transforms it into a sequence of 64 tokens (pieces on the board)
- iteratively predicts a sequence of 3 new tokens (`fr`, `to` and `up`) which, when combined, give the move to play (i.e. from square, to square and promotion, if any)
- transforms this sequence back into a UCI-formatted move (i.e. `a7a8q`)

<br/>

Current model's architecture has:
- `256` dimensions/features per token
- `8` layers (Transformers blocks)
- `8` heads per layer
- `3,751,045` total parameters

<br/>

## Installation

In order to use the UCI engine (v0.1 to v0.4), you will need the following:
- a Python runtime environment: https://www.python.org/
- for v0.1 and v0.2: some Python libraries (see `./data preparation/1. install_libraries.bat`)
- for v0.3 and v0.4: some Python libraries (see `./install/requirements.txt`)
- to download the [package](https://www.orionchess.com/download/Nostradamus-v0.1-to-v0.4.zip) containing the engines and their respective models

<br/>

Note:
- there's no need for a powerful GPU for inference!
- i.e. if you only want to use the Nostradamus engine and not to train models, you can install the CPU version of PyTorch

<br/>

## Data used for the training

Models were trained using data:
- extracted from [CCRL](https://www.computerchess.org.uk/ccrl/) games (CCRL 40/2 Archive, CCRL Blitz and CCRL 40/15)
- excluding drawn games (i.e. `1/2-1/2` result)

<br/>

This resulted in a dataset:
- of ~81M positions for v0.1 and v0.2 (as of December 2024)
- of ~86M positions for v0.3 and v0.4 (as of August 2025)

<br/>

## If you want to train your own models

In v0.1 and v0.2, source code is provided to:

- train the tokenizer, responsible for converting fenstrings and moves into sequences of "tokens"
- train the language model, responsible for predicting the most likely move for a given fenstring

<br/>

In v0.3 and v0.4, source code is also provided to:

- train the model responsible for predicting the most likely move for a given fenstring

<br/>

For v0.1 and v0.2, you'll need:

- an **NVIDIA GPU** (do not consider training on the CPU, as it would be very slow)
- the `pgn-extract` tool (https://www.cs.kent.ac.uk/people/staff/djb/pgn-extract/)
- a `games.pgn` file containing games in PGN-format, placed in a `pgn` folder (to be created)

<br/>

For v0.3 and v0.4:

- training has been also successfully performed on an **Apple Silicon CPU** (Mac mini M4 Pro 10/16, 24 GB RAM)
- you can provide several PGN files (which will be automatically combined) in the `./preparation/pgn` folder

<br/>

Some useful figures:

- for v0.1 and v0.2, consider 5-6 hours per epoch on an Nvidia RTX 4070 Ti (12GB VRAM GPU)
- for v0.3, consider 4-5 hours per epoch on the same GPU (much slower on the Mac mini)
- for v0.4, consider 6 hours per epoch on the same GPU (much slower on the Mac mini)

<br/>

## Contribute to the experiment!

If you would like to contribute, please contact me via the [talkchess.com](https://www.talkchess.com) forum!

<br/>

Ideas for the future:

- find/build a larger training dataset
- implement gradient accumulation across batches, to improve training convergence and stability
- find suitable hardware to train larger networks
- try to predict a sequence of moves

<br/>


## Copyright, licence

Copyright 2024-2025 by David Carteau. All rights reserved.

The Nostradamus UCI chess engine is licensed under the **MIT License** (see "LICENSE" and "license.txt" files).
